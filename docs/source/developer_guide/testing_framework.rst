.. _testing_framework:

``Sktime`` testing framework overview
=====================================

``sktime`` uses ``pytest`` for testing interface compliance of estimators, and correctness of code.
This page gives an overview over the tests, and introductions on how to add tests, or how to extend the testing framework.

.. contents::
   :local:

Test module architecture
------------------------

``sktime`` testing happens on three layers, roughly corresponding to the inheritance layers of estimators.

* testing interface compliance with the ``BaseObject`` and ``BaseEstimator`` specifications, in
``tests/test_all_estimators.py``
* testing interface compliance of concrete estimators with their scitype base class, for instance
``forecasting/tests/test_all_forecasters.py``
* testing individual functionality of estimators or other code, in individual files in ``tests`` folders

Module conventions are as follows:

* each module contains a ``tests`` folder, which contains tests specific to that module.
Sub-modules may also contain ``tests`` folders.
* ``tests`` folders may contain ``_config.py`` files to collect test configuration settings for that module
* generic utilities for tests are located in the module ``utils._testing``.
 Tests for these utilities should be contained in the ``utils._testing.tests`` folder.
* each test module corresponding to a learning task and estimator scitype should
 contain a test ``test_all_[name_of_scitype].py`` file that tests interface compliance of all estimators adhering to the scitype.
 For instance, ``forecasting/tests/test_all_forecasters.py``, or ``distances/tests/test_all_dist_kernels.py``.
* Learning task specific tests should not duplicate generic estimator tests in ``test_all_estimators.py``

Test code architecture
----------------------

.. _pytestuse: https://docs.pytest.org/en/6.2.x/example/index.html

``sktime`` test files should use best ``pytest`` practice such as fixtures or test parameterization where possible,
instead of custom logic, see `pytest documentation <pytestuse>`_.

Estimator tests use ``sktime``'s framework plug-in to ``pytest_generate_tests``,
which parameterizes estimator fixtures and data input scenarios.

An illustrative example
~~~~~~~~~~~~~~~~~~~~~~~

Starting with an example:

.. code-block::

   def test_fit_returns_self(estimator_instance, scenario):
      """Check that fit returns self."""
      fit_return = scenario.run(estimator_instance, method_sequence=["fit"])
      assert (
         fit_return is estimator_instance
      ), f"Estimator: {estimator_instance} does not return self when calling fit"

Here, ``estimator_instance`` and ``scenario`` fixture are looped over, assuming
values from a list of fixtures:
* ``estimator_instance`` over estimator instances, obtained from all ``sktime`` estimators via ``create_test_instances_and_names``
* ``scenario`` objects, which encodes data inputs and method call sequences to ``estimator_instance`` (explained in further detail below).

The ``scenario.run`` command is equivalent to calling ``estimator_instance.fit(**scenario_kwargs)``,
where the ``scenario_kwargs`` are generated by the ``scenario``.

It should be noted that the test is not decorated decorated with fixture parameterization,
the fixtures are instead generated by ``pytest_generate_tests``.

The reason for this is that the applicable scenarios (fixture values of ``scenario``) depend on the ``estimator_instance`` fixture,
since inputs to ``fit`` of a classifier will differ to an input to ``fit`` of a forecaster.

Parameterized fixtures
~~~~~~~~~~~~~~~~~~~~~~

Currently, the ``sktime`` testing framework parameterizes the following fixtures:

* ``estimator``: all estimator classes, inheriting from the base class of the given module.
In ``test_all_estimators``, this loops over all estimators.
* ``estimator_instance``: all estimator test instances, obtained from all ``sktime`` estimators via ``create_test_instances_and_names``
* ``scenario``: test scenarios, applicable to ``estimator`` or ``estimator_instance``.
   The scenarios are specified in ``utils/_testing/scenarios_[estimator_scitype]``.

Scenarios
~~~~~~~~~

The ``scenario`` fixtures encode arguments for method calls, and a sequence for method calls.

An example scenario specification, from ``utils/_testing/scenarios_forecasting``:

.. code-block::

   class ForecasterFitPredictUnivariateNoXLateFh(ForecasterTestScenario):
      """Fit/predict only, univariate y, no X, no fh in predict."""

      _tags = {"univariate_y": True, "fh_passed_in_fit": False}

      args = {
         "fit": {"y": _make_series(n_timepoints=20, random_state=RAND_SEED)},
         "predict": {"fh": 1},
      }
      default_method_sequence = ["fit", "predict"]

The scenario ``ForecasterFitPredictUnivariateNoXLateFh`` encodes instructions
applied to an ``estimator_instance``, via instances ``scenario``.
A call ``result = scenario.run(estimator_instance)`` will:

1. first, call ``estimator_instance.fit(y=_make_series(n_timepoints=20, random_state=RAND_SEED))``
2. then, call ``estimator_instance.predict(fh=1)`` and return the  output too ``result``.

The abstraction of "scenario" allows to easily specify multiple argument combinations across multiple methods.

The method ``run`` also has arguments that allow to override the method sequence, e.g.,
run them in a different order, or only a subset thereof.

Scenarios also provide a method ``scenario.is_applicable(estimator)``, which returns a boolean, whether
``scenario`` is applicable to ``estimator``. For instance, scenarios with univariate data are not applicable
to multivariate forecasters, and will cause exceptions in a ``fit`` method call.
Non-applicable scenarios can be filtered out in positive tests, and filtered in in negative tests.
As a default, the ``sktime`` implemented ``pytest_generate_tests`` only pass applicable scenarios.

For further details on scenarios, inspect the docstring of ``BaseScenario``.

Extending the testing module
----------------------------


Adding tests
~~~~~~~~~~~~


Adding fixtures
~~~~~~~~~~~~~~~

Adding scenarios
~~~~~~~~~~~~~~~~