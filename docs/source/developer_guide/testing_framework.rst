.. _testing_framework:

``Sktime`` testing framework overview
=====================================

``sktime`` uses ``pytest`` for testing interface compliance of estimators, and correctness of code.
This page gives an overview over the tests, and introductions on how to add tests, or how to extend the testing framework.

.. contents::
   :local:

Test module architecture
------------------------

``sktime`` testing happens on three layers, roughly corresponding to the inheritance layers of estimators.

* "package level": testing interface compliance with the ``BaseObject`` and ``BaseEstimator`` specifications, in
``tests/test_all_estimators.py``
* "module level": testing interface compliance of concrete estimators with their scitype base class, for instance
``forecasting/tests/test_all_forecasters.py``
* "low level": testing individual functionality of estimators or other code, in individual files in ``tests`` folders

Module conventions are as follows:

* each module contains a ``tests`` folder, which contains tests specific to that module.
Sub-modules may also contain ``tests`` folders.
* ``tests`` folders may contain ``_config.py`` files to collect test configuration settings for that module
* generic utilities for tests are located in the module ``utils._testing``.
 Tests for these utilities should be contained in the ``utils._testing.tests`` folder.
* each test module corresponding to a learning task and estimator scitype should contain 
module level tests in a test ``test_all_[name_of_scitype].py`` file that tests interface compliance of all estimators adhering to the scitype.
 For instance, ``forecasting/tests/test_all_forecasters.py``, or ``distances/tests/test_all_dist_kernels.py``.
* Learning task specific tests should not duplicate package level, generic estimator tests in ``test_all_estimators.py``

Test code architecture
----------------------

.. _pytestuse: https://docs.pytest.org/en/6.2.x/example/index.html

``sktime`` test files should use best ``pytest`` practice such as fixtures or test parameterization where possible,
instead of custom logic, see `pytest documentation <pytestuse>`_.

Estimator tests use ``sktime``'s framework plug-in to ``pytest_generate_tests``,
which parameterizes estimator fixtures and data input scenarios.

An illustrative example
~~~~~~~~~~~~~~~~~~~~~~~

Starting with an example:

.. code-block::

   def test_fit_returns_self(estimator_instance, scenario):
      """Check that fit returns self."""
      fit_return = scenario.run(estimator_instance, method_sequence=["fit"])
      assert (
         fit_return is estimator_instance
      ), f"Estimator: {estimator_instance} does not return self when calling fit"

Here, ``estimator_instance`` and ``scenario`` fixture are looped over, assuming
values from a list of fixtures:
* ``estimator_instance`` over estimator instances, obtained from all ``sktime`` estimators via ``create_test_instances_and_names``
* ``scenario`` objects, which encodes data inputs and method call sequences to ``estimator_instance`` (explained in further detail below).

The ``scenario.run`` command is equivalent to calling ``estimator_instance.fit(**scenario_kwargs)``,
where the ``scenario_kwargs`` are generated by the ``scenario``.

It should be noted that the test is not decorated decorated with fixture parameterization,
the fixtures are instead generated by ``pytest_generate_tests``.

The reason for this is that the applicable scenarios (fixture values of ``scenario``) depend on the ``estimator_instance`` fixture,
since inputs to ``fit`` of a classifier will differ to an input to ``fit`` of a forecaster.

Parameterized fixtures
~~~~~~~~~~~~~~~~~~~~~~

Currently, the ``sktime`` testing framework parameterizes the following fixtures:

* ``estimator``: all estimator classes, inheriting from the base class of the given module.
In ``test_all_estimators``, this loops over all estimators.
* ``estimator_instance``: all estimator test instances, obtained from all ``sktime`` estimators via ``create_test_instances_and_names``
* ``scenario``: test scenarios, applicable to ``estimator`` or ``estimator_instance``.
   The scenarios are specified in ``utils/_testing/scenarios_[estimator_scitype]``.

Scenarios
~~~~~~~~~

The ``scenario`` fixtures encode arguments for method calls, and a sequence for method calls.

An example scenario specification, from ``utils/_testing/scenarios_forecasting``:

.. code-block::

   class ForecasterFitPredictUnivariateNoXLateFh(ForecasterTestScenario):
      """Fit/predict only, univariate y, no X, no fh in predict."""

      _tags = {"univariate_y": True, "fh_passed_in_fit": False}

      args = {
         "fit": {"y": _make_series(n_timepoints=20, random_state=RAND_SEED)},
         "predict": {"fh": 1},
      }
      default_method_sequence = ["fit", "predict"]

The scenario ``ForecasterFitPredictUnivariateNoXLateFh`` encodes instructions
applied to an ``estimator_instance``, via instances ``scenario``.
A call ``result = scenario.run(estimator_instance)`` will:

1. first, call ``estimator_instance.fit(y=_make_series(n_timepoints=20, random_state=RAND_SEED))``
2. then, call ``estimator_instance.predict(fh=1)`` and return the  output too ``result``.

The abstraction of "scenario" allows to easily specify multiple argument combinations across multiple methods.

The method ``run`` also has arguments that allow to override the method sequence, e.g.,
run them in a different order, or only a subset thereof.

Scenarios also provide a method ``scenario.is_applicable(estimator)``, which returns a boolean, whether
``scenario`` is applicable to ``estimator``. For instance, scenarios with univariate data are not applicable
to multivariate forecasters, and will cause exceptions in a ``fit`` method call.
Non-applicable scenarios can be filtered out in positive tests, and filtered in in negative tests.
As a default, the ``sktime`` implemented ``pytest_generate_tests`` only pass applicable scenarios.

For further details on scenarios, inspect the docstring of ``BaseScenario``.

Extending the testing module
----------------------------

This section explains how to extend the testing module.
Depending on the primary change that is tested, the changes to the testing module will
be shallow or deep. In decreasing order of commonality:

* When adding new estimators or utiliy functionality, it usually suffices to write only
low level tests that check correctness of the estimator.
These typically use only the simplest idioms in ``pytest`` (e.g., fixture parameterization). 
Adding a new estimator will typically *not* require changes to module level tests,
as new estimators are automatically discovered and looped over by the existing tests.
* Introducing or changing base class level interface points will typically require addition of module level tests,
and addition of, or modification to scenarios with functionality specific to these interface points.
Rarely, this may require changes package level tests.
* Major interface changes or addition of modules may require writing of entire test suites,
and changes or additions to package level tests.

Adding low level tests
~~~~~~~~~~~~~~~~~~~~~~

Low level tests are "free-form" and should follow best ``pytest`` practice.
``pytest`` tests should be located in the appropriate ``tests`` folder of the module where a change is made.
Examples should be located in the docstring of the class or function added.

For an added estimator of name ``estimator_name``, the test file should be called ``test_estimator_name.py``.

Useful functionality to write tests:
* example fixture generation, via ``datatypes.get_examples``
* data format checkers in ``datatypes``: ``check_is_mtype``, ``check_is_scitype``, ``check_raise``
* miscellaneous utilities in ``utils``, especially in ``_testing``

Adding package or module level tests
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Module level tests use ``pytest_generate_tests`` to define fixtures.

The available fixtures vary per module, and are listed in the docstring of ``pytest_generate_tests``.

A new test should use these fixtures, if possible, but also can add new fixtures via ``pytest`` basic fixture functionality.

If new fixture variables are to be used throughout the module, or depend on existing fixtures,
instructions in the next section should be followed.

Where possible, scenarios should be used to simulate generic method calls (see above),
instead of creating and passing arguments directly. Scenarios will ensure consistent coverage of input argument cases.


Adding fixture variables
~~~~~~~~~~~~~~~~~~~~~~~~

One-off fixture variables can be added using ``pytest`` basic functionality.

Fixtures used throughout module or package level tests should be added to the
fixture generation process called by ``pytest_generate_tests``.



Adding or extending scenarios
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Creating tests for a new estimator type
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~